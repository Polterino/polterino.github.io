[
	{
		"question": "Which of the following are advantages of using deep neural networks over shallow ones?",
		"options": ["The number of neurons required at each layer is exponentially smaller compared to the neurons required for a shallow network",
					"The number of neurons required at each layer in a deep network may be exponentially smaller compared to the neurons required for a shallow network",
					"A deep network with N neurons per layer overfit less than a shallow network with a single layer of N neurons",
					"A deep network tend to overfit less than a wider shallow network (i.e. a shallow network with more neurons)"],
		"correctAnswer": [1, 3],
        "verified": 1
	},
	{
		"question": "Let P and Q be two probability distributions, where Q is parametric. Which of the following sentences about the Kullback-Leibler divergence is true?",
		"options": ["KL divergence is a <b>distance</b> between two probability distributions",
					"KL divergence is a measure of the dissimilarity between two probability distributions",
					"KL divergence is the same as Cross Entropy",
					"Minimising the Cross-Entropy of P w.r.t. Q is equivalent to minimising the KL dvergence between P and Q",
					"Minimising the Mean Squared Error of P w.r.t. Q is equivalent to minimising the KL dvergence between P and Q"],
		"correctAnswer": [1, 3],
        "verified": 1
	},
	{
		"question": "For which label distribution and with which loss is reasonable to adopt the <u>linear</u> activation function for the output layer?",
		"options": ["Gaussian distribution / Mean squared error",
					"Gaussian distribution / Cross-Entropy",
					"Multinoulli distribution / Cross-Entropy",
					"Multinoulli distribution / Mean squared error",
					"Bernoulli distribution / Cross-Entropy",
					"Bernoulli distribution / Mean squared error"],
		"correctAnswer": 0,
        "verified": 1
	},
	{
		"question": "For which label distribution and with which loss is reasonable to adopt the <u>softmax</u> activation function for the output layer?",
		"options": ["Gaussian distribution / Mean squared error",
					"Gaussian distribution / Cross-Entropy",
					"Multinoulli distribution / Cross-Entropy",
					"Multinoulli distribution / Mean squared error",
					"Bernoulli distribution / Cross-Entropy",
					"Bernoulli distribution / Mean squared error"],
		"correctAnswer": 2,
        "verified": 1
	},
	{
		"question": "For which label distribution and with which loss is reasonable to adopt the <u>sigmoid</u> activation function for the output layer?",
		"options": ["Gaussian distribution / Mean squared error",
					"Gaussian distribution / Cross-Entropy",
					"Multinoulli distribution / Cross-Entropy",
					"Multinoulli distribution / Mean squared error",
					"Bernoulli distribution / Cross-Entropy",
					"Bernoulli distribution / Mean squared error"],
		"correctAnswer": 4,
        "verified": 1
	},
	{
		"question": "Let D and Q be two probability distributions, where Q is parameteric. Which one of the following sentences about the Kullback-Leibler divergence is true?",
		"options": [
			"KL divergence is a <b>distance</b> between two probability distributions",
			"Minimising the Mean Squared Error between P and Q is equivalent to minimising the KL divergence between P and Q",
			"KL divergence is the same as Cross Entropy between P and Q",
			"Minimizing the Cross-Entropy between P and Q is equivalent to minimising the KL divergence",
			"none of the above"
		],
		"correctAnswer": 3,
		"verified": 1
	},
	{
		"question": "Consider a CNN layer with 10 filters of size 4x4, a stride of 2 and input images of size 6x6 on a single channel (i.e. black and white). How many multiplications (between two numbers) are performed to compute the output (feature map) of such a layer? (do not consider bias terms or activation functions)",
		"options": [
			"640",
			"545",
			"610",
			"590",
			"663"
		],
		"correctAnswer": 0,
		"motivation": "10 x 4x4 x ( (6-4)/2 + 1)<sup>2</sup>",
		"verified": 1
	},
	{
		"question": "Consider an <u>unshared</u> CNN layer (i.e. a convolution layer where the weights are not shared across different positions) with just 1 filter of size 3x3, a stride of 1, input images of size 4x4 and no padding. How many parameters are we required to train for such a layer? (do not consider bias terms or activation functions)",
		"options": [
			"36",
			"120",
			"510",
			"22",
			"356"
		],
		"correctAnswer": 0,
		"motivation": "1 x 3x3 x ( (4-3)/1 + 1)<sup>2</sup>",
		"verified": 1
	},
	{
		"question": "Multi-task learning can improve the predictive performance over single-task learning (select the correct answer)",
		"options": [
			"because there are more training data to train on",
			"because the network is more complex and thus more expressive",
			"only if the considered tasks are somehow related",
			"only if there are many tasks to use (usually at least 3)",
			"None of the above"
		],
		"correctAnswer": 2,
		"verified": 1
	},
	{
		"question": "Which one of the following sentences about Second-order optimisation methods is <u>WRONG</u>?",
		"options": [
			"They consider the Hessian of the function to optimise",
			"They are computationally slower than first-order methods",
			"If used instead of first-order gradient descent, the optimisation will converge faster in terms of number of steps needed",
			"They are not widely used in optimising neural networks because they converge to a worse solution compared to first-order methods",
			"None of the others (i.e. you think that all other sentences are correct, this is not a logic test!)"
		],
		"correctAnswer": 3,
		"verified": 1
	},
	{
		"question": "Consider a time series prediction task where, given a time sequence in input up to time t, the output should predict the value of the input at time t+1. Which architecture cannot be used to perform the task?",
		"options": [
			"A feed-forward network",
			"A recurrent network with short-cut connections from the hidden state at time t to the output at time t+1",
			"A recurrent network with feedback connection from output at time t to input at time t+1",
			"A bidirectional recurrent network",
			"A recurrent network with short-cut connections from the input to the output"
		],
		"correctAnswer": 3,
		"verified": 1
	},
	{
		"question": "Given stochastic variables x1, x2, x3, x4, and the following Markov Network with factors Ï•i: The joint probability distribution P(x1, x2, x3, x4) can be factorized as:",
		"options": [
			"a",
			"b",
			"c",
			"d",
			"e"
		],
		"correctAnswer": 3,
		"verified": 1,
		"image": "img/deep_joint_distribution.png"
	},
	{
		"question": "Suppose to have a IO-isomorphic prediction task and a RNN with the following Recurrent Network: where y is the target, L the loss function, o the RNN output, h the hidden state, x the input at time t, and the black square represents the time-shift operator q<sup>-1</sup>. U, W, Vx and Vh are weights matrices. Suppose to use back-propagation through time with mini-batch equal to 2 for training. Given input sequences composed of 2 and 4 items, respectively, how many terms should be summed up to compute the gradient of the loss with respect to U?",
		"options": [
			"13",
			"15",
			"8",
			"10",
			"20"
		],
		"correctAnswer": 0,
		"motivation": "4 + 3 + 2 + 1 + 2 + 1",
		"verified": 1,
		"image": "img/deep_rnn_1.png"
	},
	{
		"question": "Suppose to have a IO-isomorphic prediction task and a RNN with the following Recurrent Network: where y is the target, L the loss function, o the RNN output, h the hidden state, x the input at time t, and the black square represents the time-shift operator q<sup>-1</sup>. U, W, Vx and Vh are weights matrices. Suppose to use back-propagation through time with mini-batch equal to 3 for training. Given input sequences composed of 3 items, how many terms should be summed up to compute the gradient of the loss with respect to U?",
		"options": [
			"13",
			"18",
			"8",
			"10",
			"20"
		],
		"correctAnswer": 1,
		"motivation": "(1 + 2 + 3) x 3",
		"verified": 1,
		"image": "img/deep_rnn_1.png"
	},
	{
		"question": "Which of the following statements about the Universal Approximation theorem for neural networks is true?",
		"options": [
			"A neural network with at least two hidden layers and a squashing activation function can approximate any continuous function",
			"A neural network with at least one hidden layer and a squashing activation function can approximate arbitrarily well any continuous function with any number of hidden neurons",
			"A neural network with one hidden layer and a squashing activation function can approximate arbitrarily well any continuous function given enough hidden neurons",
			"The minimum number of hidden neurons required for a neural network with one hidden layer and a squashing activation function to approximate up to a given extent a continuous function is linear in the input dimension",
			"The number of hidden neurons required for a neural network with many hidden layers and a squashing activation function to approximate up to a given extent a continuous function is linear in the depth of the network"
		],
		"correctAnswer": 2,
		"verified": 1
	},
	{
		"question": "A leaky integrator with a ReLU activation function and a = 0.5 can be implemented by a GRU with:",
		"options": [
			"Activation function = ReLU, z = 0.5, r = 1, input and recurrent weights multiplied by 2",
			"Activation function = tanh, z = 0.5, r = 1",
			"Activation function = ReLU, z = 0.5, r = 0, input and recurrent weights multiplied by 0.5",
			"Activation function = ReLU, z = 0.5, r = 1, input and recurrent weights multiplied by 0.5",
			"A GRU cannot implement the described leaky integrator"
		],
		"correctAnswer": 0,
		"verified": 1
	},
	{
		"question": "Given stochastic variables X1, X2, X3, X4 and the fact that: <ul><li>X2 is conditionally independent from X3 given X4</li><li>X1 is conditionally independent from X4 given X2</li></ul> the joint probability distribution P(X1, X2, X3, X4) can be factorized as:",
		"options": [
			"P(X2) P(X4|X2) P(X1|X2) P(X3|X1, X2, X4)",
			"P(X1) P(X2) P(X3) P(X4|X3, X2)",
			"P(X3) P(X2|X1) P(X3|X1) P(X4|X2)",
			"P(X1) P(X2|X1) P(X3|X2) P(X4|X3)"
		],
		"correctAnswer": 0,
		"verified": 1
	},
	{
		"question": "Consider a 3-layers Neural Network defined as follows: <br>&emsp;&emsp;sign(W''' (W'' (W'x + b') + b'') + b'''),<br> where x is the input, b', b'', b''' are biases, W''', W'', W' are weight matrices and sign are the sign function. Which of the following sentences is TRUE:",
		"options": [
			"This 3-layers Neural Network can solve the XOR problem",
			"Adding one more Linear hidden layer will increase the expressiveness of the model",
			"The hypotheses space of this model is the same as the Perceptron",
			"If W''' has more parameters than W'' that, in turn, has more parameters than W', the network is an universal approximator",
			"None of the above."
		],
		"correctAnswer": 2,
		"verified": 1
	},
	{
		"question": "Let's consider a family of parametric probability distributions represented by a model p<sub>model</sub>(x; &Theta;) and the probability of the data of a task p<sub>data</sub>(x). Which one of the following sentences is TRUE?",
		"options": [
			"Maximum likelihood assumes a Bernoulli prior distribution of the hypothesis",
			"Minimizing the KL divergence between p<sub>data</sub> and p<sub>model</sub> corresponds to minimizing the cross-entropy between the two distributions",
			"Bayesian approach make predictions using a single point estimate of &Theta;",
			"Maximum likelihood approaches make predictions using a full probability distribution over &Theta;",
			"None of the above"
		],
		"correctAnswer": 1,
		"verified": 1
	},
	{
		"question": "Consider a CNN layer with 2 filters of size 3x3, stride 1, input images of size 4x4 and padding=\"same\" on a colour image (i.e. with three channels). How many <u>multiplications</u> (between two numbers) are performed to compute the output (feature map) of such a layer? (do not consider bias terms or activation functions)",
		"options": [
			"864",
			"216",
			"522",
			"120",
			"445",
			"288"
		],
		"correctAnswer": 0,
		"motivation": "2 x 3x3 x 4x4 x 3",
		"verified": 0
	},
	{
		"question": "Which one of the following sentences about model selection is true?",
		"options": [
			"In order to select a model (i.e. during model selection), the estimation of the model's true error is conducted using the <u>test data</u>",
			"Selecting the values of hyperparameters modifies the model's hypothesis space and/or the inductive bias",
			"The goal of model selection is to identify the set of hyper-parameters that minimizes the <u>empirical error</u>",
			"The special case k=1 of k-fold cross validation is called leave-one-out cross validation",
			"None of the above"
		],
		"correctAnswer": 1,
		"verified": 1
	},
	{
		"question": "Let P and Q be two probability distributions, where Q is parameteric. Which one of the following sentences about the Kullback-Leibler divergence is true?",
		"options": [
			"The KL divergence between P and Q is equivalent to the KL divergence between Q and P",
			"Maximizing the Mean Squared Error between P and Q is equivalent to minimising the KL divergence between the two distributions",
			"Minimizing the KL divergence between P and Q corresponds to minimizing the conditional log-likelihood between P and Q",
			"KL divergence measures how different two probability distributions are",
			"None of the above."
		],
		"correctAnswer": 3,
		"verified": 1
	},
	{
		"question": "Consider a CNN layer with 5 filters of size 3x3, stride 3, input images of size 6x6 and padding=\"valid\" on a black and white image (i.e. with one input channel). How many <u>multiplications</u> (between two numbers) are performed to compute the output (feature map) of such a layer? (do not consider bias terms or activation functions)",
		"options": [
			"180",
			"200",
			"1620",
			"490",
			"900"
		],
		"correctAnswer": 0,
		"motivation": "5 x 3x3 x ( (6-3)/3 + 1)<sup>2</sup>",
		"verified": 1
	}
]