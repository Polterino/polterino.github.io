[
    {
        "question": "According to Bayesian inference",
        "options": ["the posterior probability is proportional to the likelihood multiplied by the prior", 
					"the amount of observed data does not influence the estimate of the posteriorthe prior probability should be uniform",
					"the normalization factor represents the likelihood of the data",
					"the posterior probability can be estimated using maximum-likelihood"],
        "correctAnswer": 0
    },
    {
        "question": "In probabilistic programming",
        "options": ["The output is always a probability distribution",
					"You create a computer program that...", 
					"Another answer with computer program",
					"last answer"],
        "correctAnswer": 1
    },
	{
        "question": "Intercausal reasoning in Bayesian networks updates the estimate of a variable",
        "options": ["using information from the parents of its children",
					"using information from its parents",
					"using information from the variables in the Markov blanket",
					"using an iterative procedure",
					"using information from its children"],
        "correctAnswer": 0
    },
    {
        "question": "A universal computing machine",
        "options": ["approximates a Turing machine with a neural network",
					"can solve the symbol grounding problem through inductive inference",
					"is an ideal computing machine with continuous states",
					"is a theoretical device that can simulate any algorithm",
					"can simulate any algorithm using a finite amount of memory"],
        "correctAnswer": 3
    },
	{
        "question": "According to Innatism",
        "options": ["most of our knowledge is present at birth",
					"mental states are completely determined by brain states",
					"the nature of our knowledge is probabilistic",
					"we can generalize the property of a population from the property of a sample",
					"a rational agent can select the best action regardless of the sensory evidence"],
        "correctAnswer": 0
    },
    {
        "question": "The emergentist approach for modeling cognition",
        "options": ["emphasizes the central role of deductive reasoning in knowledge discovery",
					"is based on explicit knowledge representations",
					"argues that thinking arises from the activation dynamics of neuronal networks",
					"argues that cognition can be programmed using syntactic rules",
					"argues that intelligence can be programmed in a universal computing machine"],
        "correctAnswer": 2
    },
	{
        "question": "The algorithmic level in Marr's \"three levels of analysis\"",
        "options": ["describes how the computation can be physically implemented",
					"describes the goal of the computation",
					"describes the goal of the computation and how inputs and outputs are represented",
					"describes the goal of the algorithm and how it can be implemented",
					"describes the representation for the input and the output of the model"],
        "correctAnswer": 4
    },
	{
        "question": "In order to adjudicate between competing theories",
        "options": ["we should choose the simpler model",
					"we should choose the most transparent model",
					"verbal theories should be preferred over black-box models",
					"we should choose the model with better descriptive adequacy",
					"we should use strong inference to test the transparency of the model"],
        "correctAnswer": 3
    },
	{
        "question": "In emergentist cognitive models",
        "options": ["it is easier to attribute a meaning to each representatonal unit",
					"we assume that knowledge and reasoning are separate processes",
					"knowledge is represented in a declarative form",
					"knowledge is represented explicitly",
					"knowledge is represented by a vector of activations over basic processing units"],
        "correctAnswer": 4
    },
	{
		"question": "Elman networks",
        "options": ["use as context the previous state of the output layer",
					"use a context layer to learn spatial representations",
					"use as context the previous state of the hidden layer",
					"use lateral connections to process temporal information",
					"use bidirectionals connections between Internal output units"],
        "correctAnswer": 2
	},
	{
		"question": "In reinforcement learning",
        "options": ["the goal is to discover a causal model of the environment",
					"the goal is to maximize the value function",
					"the environment can be defined as a Markov Decision Process",
					"the agent learns from trial and error",
					"the goal is to maximize the control policy"],
        "correctAnswer": 3
	},
	{
		"question": "How can we factorize the joint distribution for the following Bayesian network?",
        "options": ["P(A,B,C,D,E) = P(A,B)P(C,D)P(E)",
					"P(A,B,C,D,E) = P(A)P(B)P(C|A,B) P(D)P (E|C)",
					"P(A,B,C,D,E) = P(A)P(B|D)P(C|A,B)P(D)P(E|C)",
					"P(A,B,C,D,E) = P(A)P(B|D)P(C|A,B)P(C,E)",
					"P(A,B,C,D,E) = P(A|C)P(B|C)P(D|B)P(C|E)"],
        "correctAnswer": 2,
		"image": "img/domanda.png"
	},
	{
		"question": "In the delta rule, the weight changes are given",
        "options": ["by the mean squared error between desired output and network output",
					"by the difference between target and output, multiplied by the input",
					"by the difference between target and output, multiplied by the gradient",
					"by the difference between input and output",
					"by the difference between target and output, multiplied by the bias"],
        "correctAnswer": 1
	},
	{
		"question": "In a restricted Boltzmann machine",
        "options": ["during the positive phase visible units are reconstructed",
					"lateral connections are present only in the hidden layer",
					"contrastive divergence is used to minimize the classification error",
					"units in the same laver are statistically independent",
					"activations of all units within a layer can be computed at the same time"],
        "correctAnswer": 4
	},
	{
		"question": "We have a conjugate prior",
        "options": ["when the prior distribution is in the same hypothesis space of the likelihood",
					"when the likelihood function is Gaussian",
					"when the prior distribution is Gaussian",
					"when the prior distribution is uniform",
					"when the posterior distribution is in the same family as the prior distribution"],
        "correctAnswer": 4
	},
	{
		"question": "Gibbs sampling",
        "options": ["generate random samples consistent with the evidence",
					"is an exact inference procedure for Bayesian networks",
					"exploits the Markov blanket to discard samples that are not consistent with the evidence",
					"use a Markov chain to iteratively approximate the target distribution",
					"exploits the Markov blanket to sample conditionally dependent variables at the same time"],
        "correctAnswer": 3
	},
	{
		"question": "In the formal neuron, the signals coming trom other neurons are",
        "options": ["multiplied by the threshold",
					"multiplied by the synaptic weights and then subtracted",
					"subtracted to the average activation",
					"multiplied by the synaptic weights and then summed",
					"added to the synaptic weights using a sigmoid activation function"],
        "correctAnswer": 3
	},
	{
		"question": "What distinguishes a multilayer feed-forward network from a perceptron?",
        "options": ["the linear separability of the input patterns",
					"the ability to learn pattern association",
					"the use of the delta rule",
					"the ability to learn non-linear functions",
					"the use of the Hebb rule"],
        "correctAnswer": 3
	},
	{
		"question": "In Boltzmann machines, during the positive phase",
        "options": ["correlations are computed between hidden units and model reconstructions",
					"the hidden units are clamped to the previous state",
					"both visible and hidden units can change their activation",
					"only hidden units can change their activation",
					"Gibbs sampling is used to reconstruct the input data"],
        "correctAnswer": 3
	},
	{
		"question": "In Boltzmann machines, during the positive phase",
        "options": ["both visible and hidden units can change their activation",
					"only hidden units can change their activation",
					"correlations are computed between all hidden units",
					"the visible units are clamped to the input data",
					"the hidden units are clamped to the previous input data"],
        "correctAnswer": 1
	},
	{
		"question": "A deep belief network",
        "options": ["learns a sequential generative model",
					"use the top-down connection to propagate the error gradients",
					"is formed by hierarchy of linear layer",
					"is formed by hierarchy of stochastic networks",
					"requires training labels that express probabilities"],
        "correctAnswer": 3
	},
	{
		"question": "Descriptive adequacy is",
        "options": ["the accuracy of the empirical data so that is suitable for modeling",
					"a transparent description of the data and the models",
					"the accuracy of the model in predicting the dataset",
					"the accuracy in describing the model architecture and parameters",
					"the accurate description and justification of all model assumptions"],
        "correctAnswer": 2
	},
	{
		"question": "According to empiricism",
        "options": ["we are born with innate ideas and concepts",
					"in order to understand human intelligence we need to conduct psycological experiments",
					"there is a clear distinction between mental processes and physical processes",
					"knowledge is discovered primarly by deduction",
					"most of our knowledge is derived by sensory experience"],
        "correctAnswer": 4
	}
]
