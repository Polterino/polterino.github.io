{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a5075a",
   "metadata": {
    "papermill": {
     "duration": 0.003455,
     "end_time": "2025-06-11T01:16:17.154022",
     "exception": false,
     "start_time": "2025-06-11T01:16:17.150567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# UNIPD - Deep Learning 2025 - Challenge 2: Weather Forecasting\n",
    "\n",
    "In this challenge, you will develop a deep learning model to forecast weather data based on historical observations collected from various stations.\n",
    "\n",
    "You are provided with a dataset consisting of daily weather measurements over time, for multiple stations. Your task is to forecast future weather values for each station over a 30-day horizon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc50c59",
   "metadata": {
    "papermill": {
     "duration": 0.00233,
     "end_time": "2025-06-11T01:16:17.159003",
     "exception": false,
     "start_time": "2025-06-11T01:16:17.156673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60ce20e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:16:17.165263Z",
     "iopub.status.busy": "2025-06-11T01:16:17.165002Z",
     "iopub.status.idle": "2025-06-11T01:16:23.157330Z",
     "shell.execute_reply": "2025-06-11T01:16:23.156498Z"
    },
    "papermill": {
     "duration": 5.996893,
     "end_time": "2025-06-11T01:16:23.158445",
     "exception": false,
     "start_time": "2025-06-11T01:16:17.161552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07118dd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-11T01:16:23.165224Z",
     "iopub.status.busy": "2025-06-11T01:16:23.164825Z",
     "iopub.status.idle": "2025-06-11T01:16:25.668481Z",
     "shell.execute_reply": "2025-06-11T01:16:25.667413Z"
    },
    "papermill": {
     "duration": 2.508496,
     "end_time": "2025-06-11T01:16:25.669939",
     "exception": false,
     "start_time": "2025-06-11T01:16:23.161443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/kaggle/input/unipd-deep-learning-2025-challenge-2/'\n",
    "data = pd.read_csv(data_dir + 'train_dataset.csv', index_col=[0, 1])\n",
    "stations = [station.values for _, station in data.groupby(level=0)]\n",
    "data_arr = np.stack(stations, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d523149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:16:25.676535Z",
     "iopub.status.busy": "2025-06-11T01:16:25.676289Z",
     "iopub.status.idle": "2025-06-11T01:16:25.681522Z",
     "shell.execute_reply": "2025-06-11T01:16:25.680555Z"
    },
    "papermill": {
     "duration": 0.010214,
     "end_time": "2025-06-11T01:16:25.683125",
     "exception": false,
     "start_time": "2025-06-11T01:16:25.672911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weather stations: 422\n",
      "Number of days of data: 695\n",
      "Number of weather variables: 76\n",
      "(422, 695, 76)\n"
     ]
    }
   ],
   "source": [
    "n_stations, n_days, n_features = data_arr.shape\n",
    "print(f\"Number of weather stations: {n_stations}\")\n",
    "print(f\"Number of days of data: {n_days}\")\n",
    "print(f\"Number of weather variables: {n_features}\")\n",
    "print(data_arr.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41eeec9",
   "metadata": {
    "papermill": {
     "duration": 0.002456,
     "end_time": "2025-06-11T01:16:25.688667",
     "exception": false,
     "start_time": "2025-06-11T01:16:25.686211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Definition and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0c8b63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:16:25.694982Z",
     "iopub.status.busy": "2025-06-11T01:16:25.694691Z",
     "iopub.status.idle": "2025-06-11T01:16:26.626362Z",
     "shell.execute_reply": "2025-06-11T01:16:26.625577Z"
    },
    "papermill": {
     "duration": 0.936616,
     "end_time": "2025-06-11T01:16:26.627791",
     "exception": false,
     "start_time": "2025-06-11T01:16:25.691175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Augmentation Functions\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def time_warp(x, scale_range=(0.8, 1.2)):\n",
    "    original_len = x.shape[0]\n",
    "    scale = np.random.uniform(*scale_range)\n",
    "    new_len = int(original_len * scale)\n",
    "\n",
    "    time_steps = np.linspace(0, original_len - 1, num=new_len)\n",
    "    interpolated = interp1d(np.arange(original_len), x, axis=0, kind='linear', fill_value='extrapolate')\n",
    "    x_warped = interpolated(time_steps)\n",
    "\n",
    "    # Back to original length via interpolation or truncation/padding\n",
    "    final_interp = interp1d(np.linspace(0, 1, num=new_len), x_warped, axis=0, fill_value='extrapolate')\n",
    "    x_final = final_interp(np.linspace(0, 1, num=original_len))\n",
    "    return x_final\n",
    "\n",
    "def permute_segments(x, n_segments=3):\n",
    "    segs = np.array_split(x, n_segments, axis=0)\n",
    "    np.random.shuffle(segs)\n",
    "    return np.concatenate(segs, axis=0)\n",
    "\n",
    "def magnitude_warp(x, sigma=0.2, knot=4):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    time = np.arange(x.shape[0])\n",
    "    warp = np.random.normal(loc=1.0, scale=sigma, size=(knot, x.shape[1]))\n",
    "    cs = CubicSpline(np.linspace(0, x.shape[0]-1, num=knot), warp, axis=0)\n",
    "    warping_curve = cs(time)\n",
    "    return x * warping_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f7f361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:16:26.635797Z",
     "iopub.status.busy": "2025-06-11T01:16:26.635364Z",
     "iopub.status.idle": "2025-06-11T01:16:26.643440Z",
     "shell.execute_reply": "2025-06-11T01:16:26.642819Z"
    },
    "papermill": {
     "duration": 0.012885,
     "end_time": "2025-06-11T01:16:26.644569",
     "exception": false,
     "start_time": "2025-06-11T01:16:26.631684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, forecast_len, normalize=True, augment=True):\n",
    "        self.samples = []\n",
    "        self.seq_len = seq_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.augment = augment\n",
    "\n",
    "        for station in data:\n",
    "            # Normalization by station\n",
    "            if normalize:\n",
    "                mu    = station.mean(axis=0)        # (n_features,)\n",
    "                sigma = station.std(axis=0) + 1e-8  # (n_features,)\n",
    "            else:\n",
    "                mu, sigma = np.zeros(station.shape[1]), np.ones(station.shape[1])\n",
    "\n",
    "            n = len(station)\n",
    "            max_i = n - seq_len - forecast_len + 1\n",
    "\n",
    "            for i in range(max_i):\n",
    "                x = station[i : i + seq_len].copy()\n",
    "                y = station[i + seq_len : i + seq_len + forecast_len].copy()\n",
    "                if normalize:\n",
    "                    x = (x - mu) / sigma\n",
    "                    y = (y - mu) / sigma\n",
    "\n",
    "                self.samples.append((x, y, mu, sigma))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y, mu, sigma = self.samples[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            # Gaussian noise\n",
    "            x += np.random.normal(0, 0.01, x.shape)\n",
    "\n",
    "            # Random scaling\n",
    "            x *= np.random.uniform(0.95, 1.05)\n",
    "\n",
    "            # Time warping\n",
    "            if np.random.rand() < 0.3:\n",
    "                x = time_warp(x)\n",
    "\n",
    "            # Permutation\n",
    "            if np.random.rand() < 0.3:\n",
    "                x = permute_segments(x)\n",
    "\n",
    "            # Magnitude warping\n",
    "            if np.random.rand() < 0.3:\n",
    "                x = magnitude_warp(x)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(x, dtype=torch.float32),\n",
    "            torch.tensor(y, dtype=torch.float32),\n",
    "            torch.tensor(mu, dtype=torch.float32),\n",
    "            torch.tensor(sigma, dtype=torch.float32),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c27688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:16:26.651180Z",
     "iopub.status.busy": "2025-06-11T01:16:26.650930Z",
     "iopub.status.idle": "2025-06-11T01:16:26.655505Z",
     "shell.execute_reply": "2025-06-11T01:16:26.654825Z"
    },
    "papermill": {
     "duration": 0.008999,
     "end_time": "2025-06-11T01:16:26.656521",
     "exception": false,
     "start_time": "2025-06-11T01:16:26.647522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_dataset(data_arr, val_ratio=0.2):\n",
    "    n_stations, n_days, n_features = data_arr.shape\n",
    "\n",
    "    # 20% of time for validation\n",
    "    split_idx = int(n_days * (1 - val_ratio))\n",
    "\n",
    "    train_data = data_arr[:, :split_idx, :]\n",
    "    val_data = data_arr[:, split_idx:, :]\n",
    "\n",
    "    print(f\"Train data shape: {train_data.shape}\")\n",
    "    print(f\"Validation data shape: {val_data.shape}\")\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac74962",
   "metadata": {
    "papermill": {
     "duration": 0.002326,
     "end_time": "2025-06-11T01:16:26.661690",
     "exception": false,
     "start_time": "2025-06-11T01:16:26.659364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a517ae28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:16:26.668488Z",
     "iopub.status.busy": "2025-06-11T01:16:26.667693Z",
     "iopub.status.idle": "2025-06-11T01:16:26.676801Z",
     "shell.execute_reply": "2025-06-11T01:16:26.675939Z"
    },
    "papermill": {
     "duration": 0.013921,
     "end_time": "2025-06-11T01:16:26.678218",
     "exception": false,
     "start_time": "2025-06-11T01:16:26.664297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size]\n",
    "        outputs, (h, c) = self.lstm(x)\n",
    "        return h, c  # return hidden and cell state\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        # x: [batch_size, 1, input_size]\n",
    "        output, (h, c) = self.lstm(x, (h, c))\n",
    "        prediction = self.fc(output)  # [batch_size, 1, output_size]\n",
    "        return prediction, h, c\n",
    "\n",
    "\n",
    "# Seq2Seq combines encoder and decoder for full forecasting\n",
    "class WeatherModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, forecast_horizon, num_layers=1, dropout=0.1):\n",
    "        super(WeatherModel, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = Decoder(input_size, hidden_size, input_size, num_layers, dropout)\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, seq_len, input_size]\n",
    "        batch_size = src.size(0)\n",
    "\n",
    "        # Encode the input sequence\n",
    "        h, c = self.encoder(src)\n",
    "\n",
    "        # First decoder input is the last input time step\n",
    "        decoder_input = src[:, -1:, :]  # [batch_size, 1, input_size]\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(self.forecast_horizon):\n",
    "            out, h, c = self.decoder(decoder_input, h, c)  # out: [batch, 1, input_size]\n",
    "            outputs.append(out)\n",
    "            decoder_input = out  # feeding back prediction\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, forecast_horizon, input_size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed90b1",
   "metadata": {
    "papermill": {
     "duration": 0.002402,
     "end_time": "2025-06-11T01:16:26.683669",
     "exception": false,
     "start_time": "2025-06-11T01:16:26.681267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b60fa14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:16:26.689799Z",
     "iopub.status.busy": "2025-06-11T01:16:26.689494Z",
     "iopub.status.idle": "2025-06-11T01:16:26.693320Z",
     "shell.execute_reply": "2025-06-11T01:16:26.692722Z"
    },
    "papermill": {
     "duration": 0.00814,
     "end_time": "2025-06-11T01:16:26.694276",
     "exception": false,
     "start_time": "2025-06-11T01:16:26.686136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_scale(data):\n",
    "    diffs = np.abs(data[:, 1:, :] - data[:, :-1, :])\n",
    "    return np.mean(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b29879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:16:26.700785Z",
     "iopub.status.busy": "2025-06-11T01:16:26.700444Z",
     "iopub.status.idle": "2025-06-11T01:16:26.710701Z",
     "shell.execute_reply": "2025-06-11T01:16:26.709707Z"
    },
    "papermill": {
     "duration": 0.015194,
     "end_time": "2025-06-11T01:16:26.712251",
     "exception": false,
     "start_time": "2025-06-11T01:16:26.697057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, naive_scale, num_epochs=50):\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5)\n",
    "\n",
    "    best_mase = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 5#10\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # ----- Training -----\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for x_b, y_b, _, _ in train_loader:\n",
    "            x_b, y_b = x_b.to(device), y_b.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x_b)\n",
    "            loss = criterion(preds, y_b)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "\n",
    "        avg_train_loss = total_loss / count\n",
    "\n",
    "        # ----- Validation -----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        vcount = 0\n",
    "        all_preds, all_targets, all_mu, all_sig = [], [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_b, y_b, mu_b, sig_b in val_loader:\n",
    "                x_b, y_b = x_b.to(device), y_b.to(device)\n",
    "                mu_b, sig_b = mu_b.to(device), sig_b.to(device)\n",
    "\n",
    "                preds = model(x_b)\n",
    "                loss = criterion(preds, y_b)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                vcount += 1\n",
    "\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_targets.append(y_b.cpu())\n",
    "                all_mu.append(mu_b.cpu())\n",
    "                all_sig.append(sig_b.cpu())\n",
    "\n",
    "        avg_val_loss = val_loss / vcount\n",
    "\n",
    "        # ----- Compute MASE -----\n",
    "        preds = torch.cat(all_preds, dim=0)\n",
    "        targets = torch.cat(all_targets, dim=0)\n",
    "        mus = torch.cat(all_mu, dim=0).unsqueeze(1)\n",
    "        sigs = torch.cat(all_sig, dim=0).unsqueeze(1)\n",
    "\n",
    "        preds_denorm = preds * sigs + mus\n",
    "        targets_denorm = targets * sigs + mus\n",
    "\n",
    "        mae = torch.abs(preds_denorm - targets_denorm).mean().item()\n",
    "        mase = mae / naive_scale\n",
    "\n",
    "        scheduler.step(mase)\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs} | TrainLoss: {avg_train_loss:.4f} | \"\n",
    "              f\"ValLoss: {avg_val_loss:.4f} | ValMASE: {mase:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "        # ----- Early Stopping -----\n",
    "        if mase < best_mase:\n",
    "            best_mase = mase\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(\"Best model saved\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best MASE: {best_mase:.4f}\")\n",
    "                break\n",
    "\n",
    "    return best_mase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c00f92",
   "metadata": {
    "papermill": {
     "duration": 0.002435,
     "end_time": "2025-06-11T01:16:26.717815",
     "exception": false,
     "start_time": "2025-06-11T01:16:26.715380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53305177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T01:16:26.724428Z",
     "iopub.status.busy": "2025-06-11T01:16:26.723645Z",
     "iopub.status.idle": "2025-06-11T02:07:03.851965Z",
     "shell.execute_reply": "2025-06-11T02:07:03.851233Z"
    },
    "papermill": {
     "duration": 3037.135209,
     "end_time": "2025-06-11T02:07:03.855528",
     "exception": false,
     "start_time": "2025-06-11T01:16:26.720319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (422, 556, 76)\n",
      "Validation data shape: (422, 139, 76)\n",
      "Forecast scale: 4.907250\n",
      "Epoch 1/80 | TrainLoss: 0.5340 | ValLoss: 0.4917 | ValMASE: 1.1985 | LR: 0.000010\n",
      "Best model saved\n",
      "Epoch 2/80 | TrainLoss: 0.4990 | ValLoss: 0.4892 | ValMASE: 1.1948 | LR: 0.000010\n",
      "Best model saved\n",
      "Epoch 3/80 | TrainLoss: 0.4977 | ValLoss: 0.4890 | ValMASE: 1.1945 | LR: 0.000010\n",
      "Best model saved\n",
      "Epoch 4/80 | TrainLoss: 0.4971 | ValLoss: 0.4888 | ValMASE: 1.1939 | LR: 0.000010\n",
      "Best model saved\n",
      "Epoch 5/80 | TrainLoss: 0.4961 | ValLoss: 0.4897 | ValMASE: 1.1970 | LR: 0.000010\n",
      "Epoch 6/80 | TrainLoss: 0.4945 | ValLoss: 0.4900 | ValMASE: 1.2002 | LR: 0.000010\n",
      "Epoch 7/80 | TrainLoss: 0.4930 | ValLoss: 0.4907 | ValMASE: 1.2052 | LR: 0.000010\n",
      "Epoch 8/80 | TrainLoss: 0.4913 | ValLoss: 0.4910 | ValMASE: 1.2091 | LR: 0.000010\n",
      "Epoch 9/80 | TrainLoss: 0.4896 | ValLoss: 0.4921 | ValMASE: 1.2144 | LR: 0.000010\n",
      "Early stopping at epoch 9. Best MASE: 1.1939\n"
     ]
    }
   ],
   "source": [
    "# General Parameters\n",
    "input_size = data_arr.shape[2]\n",
    "output_size = input_size\n",
    "batch_size = 64\n",
    "seq_length = 80   # Input sequence length\n",
    "forecast_horizon = 30  # Number of time steps to predict\n",
    "\n",
    "#-------- Dataset Initialization ---------------#\n",
    "# Splitting Dataset\n",
    "train_arr, val_arr = split_dataset(data_arr, val_ratio=0.2)\n",
    "\n",
    "# Calculate naive scale before creating datasets\n",
    "naive_scale = calculate_scale(train_arr)\n",
    "print(f\"Forecast scale: {naive_scale:.6f}\")\n",
    "\n",
    "# Create datasets with normalization\n",
    "train_dataset = WeatherDataset(train_arr, seq_length, forecast_horizon, normalize=True, augment=True)\n",
    "val_dataset = WeatherDataset(val_arr, seq_length, forecast_horizon, normalize=True, augment=False)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                        num_workers=0, pin_memory=True if device.type=='cuda' else False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                      num_workers=0, pin_memory=True if device.type=='cuda' else False)\n",
    "\n",
    "\n",
    "#----------Model and Training------------------#\n",
    "# Model\n",
    "model = WeatherModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    dropout=0.4\n",
    ").to(device)\n",
    "\n",
    "# Train\n",
    "best_mase = train_model(model, train_loader, val_loader, device, naive_scale, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78d38c",
   "metadata": {
    "papermill": {
     "duration": 0.002763,
     "end_time": "2025-06-11T02:07:03.861349",
     "exception": false,
     "start_time": "2025-06-11T02:07:03.858586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2577bcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T02:07:03.868480Z",
     "iopub.status.busy": "2025-06-11T02:07:03.868085Z",
     "iopub.status.idle": "2025-06-11T02:07:05.521013Z",
     "shell.execute_reply": "2025-06-11T02:07:05.520292Z"
    },
    "papermill": {
     "duration": 1.657659,
     "end_time": "2025-06-11T02:07:05.522015",
     "exception": false,
     "start_time": "2025-06-11T02:07:03.864356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Prepare last input sequence for each station\n",
    "x_input = data_arr[:, -seq_length:, :]\n",
    "\n",
    "# Compute per-station normalization stats\n",
    "mus = data_arr.mean(axis=1, keepdims=True)\n",
    "sigmas = data_arr.std(axis=1, keepdims=True) + 1e-8\n",
    "\n",
    "# Normalize input sequences\n",
    "x_input_norm = (x_input - mus) / sigmas\n",
    "x_input_tensor = torch.tensor(x_input_norm, dtype=torch.float32).to(device)\n",
    "\n",
    "# Predictions with normalized data\n",
    "with torch.no_grad():\n",
    "    y_pred_norm = model(x_input_tensor).cpu().numpy()  # (n_stations, forecast_len, n_features)\n",
    "\n",
    "# Denormalize predictions\n",
    "y_pred = y_pred_norm * sigmas + mus\n",
    "\n",
    "# Submissions\n",
    "n_stations, n_forecast_steps, n_features = y_pred.shape\n",
    "submission_data = []\n",
    "\n",
    "for station_id in range(n_stations):\n",
    "    for time_step in range(n_forecast_steps):\n",
    "        submission_id = f'{station_id}_{time_step}'\n",
    "        row = y_pred[station_id, time_step, :]\n",
    "        submission_data.append({\n",
    "            'id': submission_id,\n",
    "            **{f'var{i+1}': row[i] for i in range(n_features)}  # var1, var2, ...\n",
    "        })\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data).set_index('id')\n",
    "\n",
    "# 8. Save to CSV\n",
    "submission_df.to_csv(\"submission.csv\")\n",
    "\n",
    "print(\"Submission file saved as 'submission.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12429606,
     "sourceId": 102815,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3058.355123,
   "end_time": "2025-06-11T02:07:11.713013",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-11T01:16:13.357890",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
